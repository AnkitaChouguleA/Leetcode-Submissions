{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c9063e-660b-46a2-948a-e7ad943678cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, None),\n",
    "    (2, 1),\n",
    "    (3, 1),\n",
    "    (4, 2),\n",
    "    (5, 2)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"p_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d47482-6e14-4a4d-abd0-074630db6498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Initialize a SparkSession, which is the entry point for all PySpark functionality.\n",
    "# This is required to create and work with DataFrames.\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data representing the Tree table with node IDs and parent IDs.\n",
    "# A parent ID of `None` indicates the root node.\n",
    "data = [(1, None), (2, 1), (3, 1), (4, 2), (5, 2)]\n",
    "\n",
    "# Create a PySpark DataFrame from the sample data.\n",
    "df = spark.createDataFrame(data, [\"id\", \"p_id\"])\n",
    "\n",
    "# --- Step 1: Prepare a list of all unique parent nodes ---\n",
    "# We select the `p_id` column, filter out `None` values (the root has no parent),\n",
    "# and get the distinct IDs. We rename the column to \"parent_id\" for clarity in the join.\n",
    "parent_nodes = (\n",
    "    df.select(\"p_id\")\n",
    "      .where(col(\"p_id\").isNotNull())\n",
    "      .distinct()  # Important to avoid duplicate rows and improve performance\n",
    "      .withColumnRenamed(\"p_id\", \"parent_id\")\n",
    ")\n",
    "\n",
    "# --- Step 2: Join the main DataFrame with the list of parent nodes ---\n",
    "# We perform a `left` join to keep all original nodes.\n",
    "# The join condition matches a node's `id` to the `parent_id` list.\n",
    "# If a match is found, the node has children and the \"parent_id\" column\n",
    "# will be populated. If no match, it will be `null`.\n",
    "df_with_children = df.join(parent_nodes, df.id == parent_nodes.parent_id, how=\"left\")\n",
    "\n",
    "# --- Step 3: Classify each node based on the join results ---\n",
    "# We use a `when().otherwise()` expression to create the \"type\" column.\n",
    "result = df_with_children.withColumn(\n",
    "    \"type\",\n",
    "    # Condition 1: If the node's `p_id` is null, it's a \"Root\".\n",
    "    when(col(\"p_id\").isNull(), \"Root\")\n",
    "    # Condition 2: If the `parent_id` from the join is not null, the node is a parent\n",
    "    #              and thus an \"Inner\" node.\n",
    "    .when(col(\"parent_id\").isNotNull(), \"Inner\")\n",
    "    # Condition 3: Otherwise, if neither of the above conditions is met, it's a \"Leaf\".\n",
    "    #              This means it has a parent (`p_id` is not null) but no children\n",
    "    #              (`parent_id` is null).\n",
    "    .otherwise(\"Leaf\")\n",
    ") \\\n",
    ".select(df.id.alias(\"id\"), \"type\") \\\n",
    ".orderBy(\"id\") # Order the final result by ID for a clean, consistent output.\n",
    "\n",
    "# Display the final DataFrame showing the ID and its type.\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57391f2-8edd-47d9-aa6f-8a28bfecc8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"p_id\": [None, 1, 1, 2, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Identify nodes that are parents (appear in p_id column)\n",
    "parents = set(df[\"p_id\"].dropna())\n",
    "\n",
    "# Step 2: Classify each node\n",
    "def get_node_type(row):\n",
    "    if pd.isna(row[\"p_id\"]):\n",
    "        return \"Root\"\n",
    "    elif row[\"id\"] in parents:\n",
    "        return \"Inner\"\n",
    "    else:\n",
    "        return \"Leaf\"\n",
    "\n",
    "df[\"type\"] = df.apply(get_node_type, axis=1)\n",
    "\n",
    "print(df[[\"id\", \"type\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2090d29-55c4-4a81-bddd-d88e12449843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "608. Tree Node pyspark sln",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
